{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from data import CocoDatasetManager\n",
    "#from collections import Counter\n",
    "from backbone import ClassificationRCNN\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import UnidentifiedImageError\n",
    "import collections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize images to fit the model input\n",
    "    transforms.ToTensor(),          # Convert images to PyTorch tensors\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize with ImageNet's mean and std\n",
    "])\n",
    "\n",
    "# Load the dataset\n",
    "train_dataset = datasets.ImageFolder(root='/home/amanandhar/Transferlearning/RCNN-classification/src/data/train', transform=transform)\n",
    "\n",
    "# Create a DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['airplane', 'apple', 'backpack', 'banana', 'baseball bat', 'baseball glove', 'bear', 'bed', 'bench', 'bicycle', 'bird', 'boat', 'book', 'bottle', 'bowl', 'broccoli', 'bus', 'cake', 'car', 'carrot', 'cat', 'cell phone', 'chair', 'clock', 'couch', 'cow', 'cup', 'dining table', 'dog', 'donut', 'elephant', 'fire hydrant', 'fork', 'frisbee', 'giraffe', 'hair drier', 'handbag', 'horse', 'hot dog', 'keyboard', 'kite', 'knife', 'laptop', 'microwave', 'motorcycle', 'mouse', 'orange', 'oven', 'parking meter', 'person', 'pizza', 'potted plant', 'refrigerator', 'remote', 'sandwich', 'scissors', 'sheep', 'sink', 'skateboard', 'skis', 'snowboard', 'spoon', 'sports ball', 'stop sign', 'suitcase', 'surfboard', 'teddy bear', 'tennis racket', 'tie', 'toaster', 'toilet', 'toothbrush', 'traffic light', 'truck', 'tv', 'umbrella', 'vase', 'wine glass', 'zebra']\n",
      "Number of classes: 79\n"
     ]
    }
   ],
   "source": [
    "dataset = train_loader.dataset\n",
    "classes = dataset.classes\n",
    "num_classes = len(classes)\n",
    "\n",
    "print(f\"Classes: {classes}\")\n",
    "print(f\"Number of classes: {num_classes}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amanandhar/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/amanandhar/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_V2_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_V2_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have defined a model, loss function, and optimizer\n",
    "model = ClassificationRCNN(num_classes=79)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training for 50 epochs.\n",
      "Epoch 1 started.\n",
      "Epoch [1/50], Batch 0, Loss: 3.2915\n",
      "Epoch [1/50], Batch 10, Loss: 2.6532\n",
      "Epoch [1/50], Batch 20, Loss: 2.9984\n",
      "Epoch [1/50], Batch 30, Loss: 3.4111\n",
      "Epoch [1/50], Batch 40, Loss: 2.9879\n",
      "Epoch [1/50], Batch 50, Loss: 2.8945\n",
      "Epoch [1/50], Batch 60, Loss: 3.1745\n",
      "Epoch [1/50], Batch 70, Loss: 2.8458\n",
      "Epoch [1/50], Batch 80, Loss: 3.2385\n",
      "Epoch [1/50], Batch 90, Loss: 3.4533\n",
      "Epoch [1/50], Batch 100, Loss: 2.7143\n",
      "Epoch [1/50], Batch 110, Loss: 2.7121\n",
      "Epoch [1/50], Batch 120, Loss: 2.8497\n",
      "Epoch [1/50], Batch 130, Loss: 2.7998\n",
      "Epoch [1/50], Batch 140, Loss: 3.1809\n",
      "Epoch [1/50], Batch 150, Loss: 2.5038\n",
      "Epoch [1/50], Batch 160, Loss: 2.9248\n",
      "Epoch [1/50], Batch 170, Loss: 2.6116\n",
      "Epoch [1/50], Batch 180, Loss: 3.3620\n",
      "Epoch [1/50], Batch 190, Loss: 2.5513\n",
      "Epoch [1/50], Batch 200, Loss: 2.7174\n",
      "Epoch [1/50], Batch 210, Loss: 2.9149\n",
      "Epoch [1/50], Batch 220, Loss: 3.0865\n",
      "Epoch [1/50], Batch 230, Loss: 3.3078\n",
      "Epoch [1/50], Batch 240, Loss: 3.3203\n",
      "Epoch [1/50], Batch 250, Loss: 2.9904\n",
      "Epoch [1/50], Batch 260, Loss: 2.8403\n",
      "Epoch [1/50], Batch 270, Loss: 2.5491\n",
      "Epoch [1/50], Batch 280, Loss: 2.5463\n",
      "Epoch [1/50], Batch 290, Loss: 3.1607\n",
      "Epoch [1/50] completed, Loss: 2.9733\n",
      "Epoch 2 started.\n",
      "Epoch [2/50], Batch 0, Loss: 2.6527\n",
      "Epoch [2/50], Batch 10, Loss: 2.5159\n",
      "Epoch [2/50], Batch 20, Loss: 2.4811\n",
      "Epoch [2/50], Batch 30, Loss: 3.2078\n",
      "Epoch [2/50], Batch 40, Loss: 2.7619\n",
      "Epoch [2/50], Batch 50, Loss: 3.0161\n",
      "Epoch [2/50], Batch 60, Loss: 2.5630\n",
      "Epoch [2/50], Batch 70, Loss: 2.8990\n",
      "Epoch [2/50], Batch 80, Loss: 2.9295\n",
      "Epoch [2/50], Batch 90, Loss: 2.8659\n",
      "Epoch [2/50], Batch 100, Loss: 2.8368\n",
      "Epoch [2/50], Batch 110, Loss: 2.5867\n",
      "Epoch [2/50], Batch 120, Loss: 2.1863\n",
      "Epoch [2/50], Batch 130, Loss: 2.5675\n",
      "Epoch [2/50], Batch 140, Loss: 2.8107\n",
      "Epoch [2/50], Batch 150, Loss: 2.3232\n",
      "Epoch [2/50], Batch 160, Loss: 2.4890\n",
      "Epoch [2/50], Batch 170, Loss: 2.2052\n",
      "Epoch [2/50], Batch 180, Loss: 2.4227\n",
      "Epoch [2/50], Batch 190, Loss: 2.5464\n",
      "Epoch [2/50], Batch 200, Loss: 2.7826\n",
      "Epoch [2/50], Batch 210, Loss: 2.3709\n",
      "Epoch [2/50], Batch 220, Loss: 2.2728\n",
      "Epoch [2/50], Batch 230, Loss: 3.5496\n",
      "Epoch [2/50], Batch 240, Loss: 2.3931\n",
      "Epoch [2/50], Batch 250, Loss: 2.5307\n",
      "Epoch [2/50], Batch 260, Loss: 2.2179\n",
      "Epoch [2/50], Batch 270, Loss: 2.2491\n",
      "Epoch [2/50], Batch 280, Loss: 2.6408\n",
      "Epoch [2/50], Batch 290, Loss: 2.5803\n",
      "Epoch [2/50] completed, Loss: 2.1346\n",
      "Epoch 3 started.\n",
      "Epoch [3/50], Batch 0, Loss: 2.3922\n",
      "Epoch [3/50], Batch 10, Loss: 2.3406\n",
      "Epoch [3/50], Batch 20, Loss: 2.3813\n",
      "Epoch [3/50], Batch 30, Loss: 2.8298\n",
      "Epoch [3/50], Batch 40, Loss: 2.5086\n",
      "Epoch [3/50], Batch 50, Loss: 3.0577\n",
      "Epoch [3/50], Batch 60, Loss: 2.1933\n",
      "Epoch [3/50], Batch 70, Loss: 2.8661\n",
      "Epoch [3/50], Batch 80, Loss: 2.4150\n",
      "Epoch [3/50], Batch 90, Loss: 1.7079\n",
      "Epoch [3/50], Batch 100, Loss: 2.3006\n",
      "Epoch [3/50], Batch 110, Loss: 2.7164\n",
      "Epoch [3/50], Batch 120, Loss: 2.4058\n",
      "Epoch [3/50], Batch 130, Loss: 2.5291\n",
      "Epoch [3/50], Batch 140, Loss: 2.4309\n",
      "Epoch [3/50], Batch 150, Loss: 2.7497\n",
      "Epoch [3/50], Batch 160, Loss: 2.2424\n",
      "Epoch [3/50], Batch 170, Loss: 2.4448\n",
      "Epoch [3/50], Batch 180, Loss: 2.9434\n",
      "Epoch [3/50], Batch 190, Loss: 2.7130\n",
      "Epoch [3/50], Batch 200, Loss: 2.6889\n",
      "Epoch [3/50], Batch 210, Loss: 2.9988\n",
      "Epoch [3/50], Batch 220, Loss: 2.1610\n",
      "Epoch [3/50], Batch 230, Loss: 1.8709\n",
      "Epoch [3/50], Batch 240, Loss: 2.2235\n",
      "Epoch [3/50], Batch 250, Loss: 2.1589\n",
      "Epoch [3/50], Batch 260, Loss: 2.2574\n",
      "Epoch [3/50], Batch 270, Loss: 2.6553\n",
      "Epoch [3/50], Batch 280, Loss: 2.5974\n",
      "Epoch [3/50], Batch 290, Loss: 2.1553\n",
      "Epoch [3/50] completed, Loss: 2.0139\n",
      "Epoch 4 started.\n",
      "Epoch [4/50], Batch 0, Loss: 1.7825\n",
      "Epoch [4/50], Batch 10, Loss: 2.1531\n",
      "Epoch [4/50], Batch 20, Loss: 2.3256\n",
      "Epoch [4/50], Batch 30, Loss: 2.1786\n",
      "Epoch [4/50], Batch 40, Loss: 2.3811\n",
      "Epoch [4/50], Batch 50, Loss: 1.8194\n",
      "Epoch [4/50], Batch 60, Loss: 2.0792\n",
      "Epoch [4/50], Batch 70, Loss: 2.3749\n",
      "Epoch [4/50], Batch 80, Loss: 2.3920\n",
      "Epoch [4/50], Batch 90, Loss: 2.1862\n",
      "Epoch [4/50], Batch 100, Loss: 2.5245\n",
      "Epoch [4/50], Batch 110, Loss: 2.1425\n",
      "Epoch [4/50], Batch 120, Loss: 2.1628\n",
      "Epoch [4/50], Batch 130, Loss: 2.4650\n",
      "Epoch [4/50], Batch 140, Loss: 2.3795\n",
      "Epoch [4/50], Batch 150, Loss: 2.0630\n",
      "Epoch [4/50], Batch 160, Loss: 2.2456\n",
      "Epoch [4/50], Batch 170, Loss: 2.4111\n",
      "Epoch [4/50], Batch 180, Loss: 2.0609\n",
      "Epoch [4/50], Batch 190, Loss: 2.3929\n",
      "Epoch [4/50], Batch 200, Loss: 2.2920\n",
      "Epoch [4/50], Batch 210, Loss: 2.0325\n",
      "Epoch [4/50], Batch 220, Loss: 2.7589\n",
      "Epoch [4/50], Batch 230, Loss: 1.7509\n",
      "Epoch [4/50], Batch 240, Loss: 1.6991\n",
      "Epoch [4/50], Batch 250, Loss: 3.0390\n",
      "Epoch [4/50], Batch 260, Loss: 2.5959\n",
      "Epoch [4/50], Batch 270, Loss: 2.8258\n",
      "Epoch [4/50], Batch 280, Loss: 2.5572\n",
      "Epoch [4/50], Batch 290, Loss: 2.4849\n",
      "Epoch [4/50] completed, Loss: 2.2910\n",
      "Epoch 5 started.\n",
      "Epoch [5/50], Batch 0, Loss: 2.2956\n",
      "Epoch [5/50], Batch 10, Loss: 2.1892\n",
      "Epoch [5/50], Batch 20, Loss: 2.1411\n",
      "Epoch [5/50], Batch 30, Loss: 2.2016\n",
      "Epoch [5/50], Batch 40, Loss: 2.2834\n",
      "Epoch [5/50], Batch 50, Loss: 2.6097\n",
      "Epoch [5/50], Batch 60, Loss: 1.6278\n",
      "Epoch [5/50], Batch 70, Loss: 1.8053\n",
      "Epoch [5/50], Batch 80, Loss: 2.1726\n",
      "Epoch [5/50], Batch 90, Loss: 2.1514\n",
      "Epoch [5/50], Batch 100, Loss: 2.4196\n",
      "Epoch [5/50], Batch 110, Loss: 2.2932\n",
      "Epoch [5/50], Batch 120, Loss: 2.1318\n",
      "Epoch [5/50], Batch 130, Loss: 2.6000\n",
      "Epoch [5/50], Batch 140, Loss: 2.1536\n",
      "Epoch [5/50], Batch 150, Loss: 2.4874\n",
      "Epoch [5/50], Batch 160, Loss: 2.4183\n",
      "Epoch [5/50], Batch 170, Loss: 2.0783\n",
      "Epoch [5/50], Batch 180, Loss: 2.2275\n",
      "Epoch [5/50], Batch 190, Loss: 1.9881\n",
      "Epoch [5/50], Batch 200, Loss: 2.4140\n",
      "Epoch [5/50], Batch 210, Loss: 2.2328\n",
      "Epoch [5/50], Batch 220, Loss: 2.3002\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/amanandhar/Transferlearning/RCNN-classification/src/test.ipynb Cell 5\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/amanandhar/Transferlearning/RCNN-classification/src/test.ipynb#X12sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/amanandhar/Transferlearning/RCNN-classification/src/test.ipynb#X12sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/amanandhar/Transferlearning/RCNN-classification/src/test.ipynb#X12sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m optimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/amanandhar/Transferlearning/RCNN-classification/src/test.ipynb#X12sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39mif\u001b[39;00m i \u001b[39m%\u001b[39m \u001b[39m10\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:  \u001b[39m# Print every 10 batches\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/amanandhar/Transferlearning/RCNN-classification/src/test.ipynb#X12sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch [\u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mnum_epochs\u001b[39m}\u001b[39;00m\u001b[39m], Batch \u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m, Loss: \u001b[39m\u001b[39m{\u001b[39;00mloss\u001b[39m.\u001b[39mitem()\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/optimizer.py:280\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    277\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m}\u001b[39;00m\u001b[39m must return None or a tuple of (new_args, new_kwargs),\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m                                \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m{\u001b[39;00mresult\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 280\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    281\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    283\u001b[0m \u001b[39m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/optimizer.py:33\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     32\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m---> 33\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     34\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     35\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/adam.py:141\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    130\u001b[0m     beta1, beta2 \u001b[39m=\u001b[39m group[\u001b[39m'\u001b[39m\u001b[39mbetas\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m    132\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_group(\n\u001b[1;32m    133\u001b[0m         group,\n\u001b[1;32m    134\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    138\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    139\u001b[0m         state_steps)\n\u001b[0;32m--> 141\u001b[0m     adam(\n\u001b[1;32m    142\u001b[0m         params_with_grad,\n\u001b[1;32m    143\u001b[0m         grads,\n\u001b[1;32m    144\u001b[0m         exp_avgs,\n\u001b[1;32m    145\u001b[0m         exp_avg_sqs,\n\u001b[1;32m    146\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    147\u001b[0m         state_steps,\n\u001b[1;32m    148\u001b[0m         amsgrad\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mamsgrad\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    149\u001b[0m         beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    150\u001b[0m         beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    151\u001b[0m         lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    152\u001b[0m         weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    153\u001b[0m         eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    154\u001b[0m         maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    155\u001b[0m         foreach\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mforeach\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    156\u001b[0m         capturable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mcapturable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    157\u001b[0m         differentiable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mdifferentiable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    158\u001b[0m         fused\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mfused\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    159\u001b[0m         grad_scale\u001b[39m=\u001b[39;49m\u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mgrad_scale\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m    160\u001b[0m         found_inf\u001b[39m=\u001b[39;49m\u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mfound_inf\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m    161\u001b[0m     )\n\u001b[1;32m    163\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/adam.py:281\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    279\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 281\u001b[0m func(params,\n\u001b[1;32m    282\u001b[0m      grads,\n\u001b[1;32m    283\u001b[0m      exp_avgs,\n\u001b[1;32m    284\u001b[0m      exp_avg_sqs,\n\u001b[1;32m    285\u001b[0m      max_exp_avg_sqs,\n\u001b[1;32m    286\u001b[0m      state_steps,\n\u001b[1;32m    287\u001b[0m      amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[1;32m    288\u001b[0m      beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    289\u001b[0m      beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    290\u001b[0m      lr\u001b[39m=\u001b[39;49mlr,\n\u001b[1;32m    291\u001b[0m      weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[1;32m    292\u001b[0m      eps\u001b[39m=\u001b[39;49meps,\n\u001b[1;32m    293\u001b[0m      maximize\u001b[39m=\u001b[39;49mmaximize,\n\u001b[1;32m    294\u001b[0m      capturable\u001b[39m=\u001b[39;49mcapturable,\n\u001b[1;32m    295\u001b[0m      differentiable\u001b[39m=\u001b[39;49mdifferentiable,\n\u001b[1;32m    296\u001b[0m      grad_scale\u001b[39m=\u001b[39;49mgrad_scale,\n\u001b[1;32m    297\u001b[0m      found_inf\u001b[39m=\u001b[39;49mfound_inf)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/adam.py:344\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    341\u001b[0m     param \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mview_as_real(param)\n\u001b[1;32m    343\u001b[0m \u001b[39m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[0;32m--> 344\u001b[0m exp_avg\u001b[39m.\u001b[39;49mmul_(beta1)\u001b[39m.\u001b[39;49madd_(grad, alpha\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m \u001b[39m-\u001b[39;49m beta1)\n\u001b[1;32m    345\u001b[0m exp_avg_sq\u001b[39m.\u001b[39mmul_(beta2)\u001b[39m.\u001b[39maddcmul_(grad, grad\u001b[39m.\u001b[39mconj(), value\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m beta2)\n\u001b[1;32m    347\u001b[0m \u001b[39mif\u001b[39;00m capturable \u001b[39mor\u001b[39;00m differentiable:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 50\n",
    "\n",
    "if len(train_loader) == 0:\n",
    "    print(\"The DataLoader is empty. Check your dataset.\")\n",
    "else:\n",
    "    print(f\"Starting training for {num_epochs} epochs.\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch+1} started.\")\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        try:\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if i % 10 == 0:  # Print every 10 batches\n",
    "                print(f\"Epoch [{epoch+1}/{num_epochs}], Batch {i}, Loss: {loss.item():.4f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}] completed, Loss: {loss.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
